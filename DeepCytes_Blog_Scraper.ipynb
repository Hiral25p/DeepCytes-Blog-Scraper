{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OOEL9GZ0bUo"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "import gspread\n",
        "from google.oauth2.service_account import Credentials\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# API Key for NewsAPI\n",
        "API_KEY = 'a2ba8940e94440a482a79029f460d29c' #'679059ca4e8e4e08b6ae2ddfd3b2bc9b'\n",
        "\n",
        "# Cybersecurity-specific news sources\n",
        "CYBERSECURITY_SOURCES = [\n",
        "    'wired', 'techcrunch', 'engadget', 'ars-technica', 'the-verge',\n",
        "    'bloomberg', 'hacker-news', 'recode', 'next-big-future',\n",
        "    'bbc-news', 'cnn', 'reuters', 'the-wall-street-journal',\n",
        "    'techradar', 'business-insider', 'the-washington-post',\n",
        "    'new-scientist', 'fortune', 'les-echos', 'handelsblatt',\n",
        "    'wirtschafts-woche', 'xinhua-net', 'financial-post',\n",
        "    'axios', 'crypto-coins-news', 'mashable', 'the-next-web',\n",
        "    'the-times-of-india', 'politico', 'vice-news',\n",
        "    'national-review', 'the-hindu', 'google-news-in',\n",
        "    'cbc-news', 'fox-news', 'le-monde', 'rt', 'google-news'\n",
        "]\n",
        "\n",
        "# Define the date range\n",
        "today = date.today()\n",
        "thirty_days_ago = today - timedelta(days=30)\n",
        "\n",
        "# List of keywords for cybersecurity-related topics\n",
        "KEYWORDS = ['cybersecurity', 'data breach', 'ransomware', 'APT', 'cyber attack', 'malware', 'phishing', 'DDoS']\n",
        "\n",
        "# Function to fetch cybersecurity articles for each keyword\n",
        "# Define the date range with an inclusive 'to' date\n",
        "today = date.today()\n",
        "thirty_days_ago = today - timedelta(days=30)\n",
        "inclusive_today = today + timedelta(days=1)  # To include articles published today\n",
        "\n",
        "# Updated function to fetch articles with an inclusive date range\n",
        "def fetch_cybersecurity_articles_by_keywords(channels, keywords, days):\n",
        "    all_articles = []\n",
        "    cutoff_date = today - timedelta(days=days)\n",
        "\n",
        "    for keyword in keywords:\n",
        "        for source in channels:\n",
        "            url = (f'https://newsapi.org/v2/everything?q={keyword}&sources={source}'\n",
        "                   f'&from={cutoff_date}&to={inclusive_today}'  # Adjusted to include today's articles\n",
        "                   f'&sortBy=relevancy&language=en&apiKey={API_KEY}')\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                articles = response.json().get('articles', [])\n",
        "                for article in articles:\n",
        "                    all_articles.append({\n",
        "                        'Date': article.get('publishedAt')[:10],\n",
        "                        'Headline': article.get('title'),\n",
        "                        'Description': article.get('description'),\n",
        "                        'Key Highlights': article.get('content', '')[:200],\n",
        "                        'Link': article.get('url'),\n",
        "                        'Source Name': article.get('source', {}).get('name', 'Unknown Source'),\n",
        "                        'Blog': '',\n",
        "                        'Blog Writer': ''\n",
        "                    })\n",
        "            else:\n",
        "                print(f\"Failed to fetch from {source} for keyword {keyword}: {response.status_code} - {response.json().get('message', 'No error message available')}\")\n",
        "    return all_articles\n",
        "\n",
        "# Filter articles to ensure they are within the desired 30-day range before saving\n",
        "df = pd.DataFrame(combined_articles, columns=[\n",
        "    'Date', 'Headline', 'Description', 'Key Highlights', 'Link', 'Source Name', 'Blog', 'Blog Writer'\n",
        "])\n",
        "\n",
        "# Convert 'Date' column to datetime and filter by the last 30 days\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "df = df[(df['Date'] >= pd.to_datetime(thirty_days_ago)) & (df['Date'] <= pd.to_datetime(today))]\n",
        "\n",
        "# Drop rows with NaN values in the 'Date' column and sort by Date\n",
        "df.dropna(subset=['Date'], inplace=True)\n",
        "df = df.sort_values(by='Date', ascending=True)\n",
        "\n",
        "\n",
        "# Scrape GB Hackers\n",
        "\"\"\"\n",
        "def scrape_gbhackers():\n",
        "    url = 'https://gbhackers.com/'\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    gbhackers_articles = []\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        articles = soup.find_all('div', class_='td-module-container')\n",
        "\n",
        "        for article in articles:\n",
        "            title_tag = article.find('h3')\n",
        "            if title_tag:\n",
        "                link_tag = title_tag.find('a')\n",
        "                title = title_tag.get_text(strip=True) if title_tag else 'No Title'\n",
        "                link = link_tag['href'] if link_tag else 'No Link'\n",
        "                description_tag = article.find('div', class_='td-excerpt')\n",
        "                description = description_tag.get_text(strip=True) if description_tag else \"No description available\"\n",
        "\n",
        "                gbhackers_articles.append({\n",
        "                    'Date': today.strftime('%Y-%m-%d'),\n",
        "                    'Headline': title,\n",
        "                    'Description': description,\n",
        "                    'Key Highlights': description[:200],\n",
        "                    'Link': link,\n",
        "                    'Source Name': 'GBHackers',\n",
        "                    'Blog': '',\n",
        "                    'Blog Writer': ''\n",
        "                })\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "\n",
        "    return gbhackers_articles\n",
        "\"\"\"\n",
        "# Scrape articles from The Cyber Express\n",
        "\"\"\"\n",
        "def scrape_cyber_express():\n",
        "    url = 'https://thecyberexpress.com/'\n",
        "    response = requests.get(url)\n",
        "    article_data = []\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='jeg_post')\n",
        "\n",
        "        for article in articles:\n",
        "            try:\n",
        "                title_tag = article.find('h3', class_='jeg_post_title')\n",
        "                title = title_tag.text.strip() if title_tag else 'No Title'\n",
        "                link_tag = title_tag.find('a')\n",
        "                article_url = link_tag['href'] if link_tag else 'No URL'\n",
        "                description_tag = article.find('div', class_='jeg_post_excerpt')\n",
        "                description = description_tag.text.strip() if description_tag else 'No Description'\n",
        "                date_tag = article.find('div', class_='jeg_meta_date')\n",
        "                pub_date = date_tag.text.strip() if date_tag else today.strftime('%Y-%m-%d')\n",
        "                author_tag = article.find('div', class_='jeg_meta_author')\n",
        "                author = author_tag.text.strip() if author_tag else 'No Author'\n",
        "\n",
        "                article_data.append({\n",
        "                    'Date': pub_date,\n",
        "                    'Headline': title,\n",
        "                    'Description': description,\n",
        "                    'Key Highlights': '',\n",
        "                    'Link': article_url,\n",
        "                    'Source Name': 'The Cyber Express',\n",
        "                    'Blog': '',\n",
        "                    'Blog Writer': author\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error while processing article: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "\n",
        "    return article_data\n",
        "\"\"\"\n",
        "\n",
        "# Scrape articles from Kaspersky Labs\n",
        "\"\"\"\n",
        "def scrape_kaspersky_labs():\n",
        "    url = \"https://www.kaspersky.com/blog\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    kaspersky_articles = []\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='c-card')\n",
        "\n",
        "        for article in articles:\n",
        "            try:\n",
        "                title_tag = article.find('h3', class_='c-card__title')\n",
        "                title = title_tag.text.strip() if title_tag else 'No title found'\n",
        "                link_tag = title_tag.find('a') if title_tag else None\n",
        "                link = link_tag['href'] if link_tag and 'href' in link_tag.attrs else 'No link found'\n",
        "                description_tag = article.find('div', class_='c-card__desc')\n",
        "                description = description_tag.text.strip() if description_tag else 'No description found'\n",
        "\n",
        "                kaspersky_articles.append({\n",
        "                    'Date': today.strftime('%Y-%m-%d'),\n",
        "                    'Headline': title,\n",
        "                    'Description': description,\n",
        "                    'Key Highlights': description[:200],\n",
        "                    'Link': link,\n",
        "                    'Source Name': 'Kaspersky Labs',\n",
        "                    'Blog': '',\n",
        "                    'Blog Writer': ''\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error while processing article: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to retrieve content. Status Code: {response.status_code}\")\n",
        "\n",
        "    return kaspersky_articles\n",
        "\"\"\"\n",
        "# Scrape articles from SentinelOne Labs\n",
        "\"\"\"\n",
        "def scrape_sentinelone_labs():\n",
        "    url = 'https://www.sentinelone.com/labs/'\n",
        "    response = requests.get(url)\n",
        "    sentinelone_articles = []\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        articles = soup.find_all('article')\n",
        "\n",
        "        for article in articles:\n",
        "            try:\n",
        "                title = article.find('h2')\n",
        "                title_text = title.text.strip() if title else 'No Title'\n",
        "                link = article.find('a', href=True)\n",
        "                article_link = link['href'] if link else 'No Link'\n",
        "                description = article.find('p').text.strip() if article.find('p') else 'No Description'\n",
        "\n",
        "                sentinelone_articles.append({\n",
        "                    'Date': today.strftime('%Y-%m-%d'),\n",
        "                    'Headline': title_text,\n",
        "                    'Description': description,\n",
        "                    'Key Highlights': description[:200],\n",
        "                    'Link': article_link,\n",
        "                    'Source Name': 'SentinelOne Labs',\n",
        "                    'Blog': '',\n",
        "                    'Blog Writer': ''\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error while processing article: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to retrieve page with status code {response.status_code}\")\n",
        "\n",
        "    return sentinelone_articles\n",
        "\"\"\"\n",
        "\n",
        "# Combine all articles from different sources\n",
        "def combine_articles():\n",
        "    articles_from_api = fetch_cybersecurity_articles_by_keywords(CYBERSECURITY_SOURCES, KEYWORDS, 30)\n",
        "   #articles_from_gbhackers = scrape_gbhackers()\n",
        "    #articles_from_cyber_express = scrape_cyber_express()\n",
        "    #articles_from_kaspersky = scrape_kaspersky_labs()\n",
        "   # articles_from_sentinelone = scrape_sentinelone_labs()\n",
        "\n",
        "    all_articles = (articles_from_api) #+ articles_from_gbhackers + articles_from_cyber_express +\n",
        "                    #articles_from_kaspersky + articles_from_sentinelone)\n",
        "\n",
        "    # Debug: Print out the number of articles fetched from each source\n",
        "    print(f\"Fetched {len(articles_from_api)} articles from NewsAPI.\")\n",
        "    \"\"\"\n",
        "    print(f\"Fetched {len(articles_from_gbhackers)} articles from GBHackers.\")\n",
        "    print(f\"Fetched {len(articles_from_cyber_express)} articles from The Cyber Express.\")\n",
        "    print(f\"Fetched {len(articles_from_kaspersky)} articles from Kaspersky Labs.\")\n",
        "    print(f\"Fetched {len(articles_from_sentinelone)} articles from SentinelOne Labs.\")\n",
        "    \"\"\"\n",
        "\n",
        "    return all_articles\n",
        "\n",
        "\n",
        "# Validate articles using specified keywords\n",
        "# Validate articles using specified keywords\n",
        "def validate_articles(articles):\n",
        "    validated_articles = []\n",
        "    keywords = \"|\".join(KEYWORDS).lower()  # Use KEYWORDS instead of validation_keywords\n",
        "\n",
        "    for article in articles:\n",
        "        # Allow articles that match any keyword in the headline or description\n",
        "        if pd.notnull(article['Headline']) and pd.notnull(article['Description']):\n",
        "            text = f\"{article['Headline']} {article['Description']}\".lower()\n",
        "            if any(kw.lower() in text for kw in KEYWORDS):\n",
        "                validated_articles.append(article)\n",
        "\n",
        "    return validated_articles\n",
        "\n",
        "\n",
        "# Save data to a CSV file to maintain records\n",
        "def save_to_csv(df, csv_filename):\n",
        "    # Append data to the CSV file (or create it if it doesn't exist)\n",
        "    try:\n",
        "        existing_df = pd.read_csv(csv_filename)  # Read existing data\n",
        "        df = pd.concat([existing_df, df]).drop_duplicates(subset=['Link']).reset_index(drop=True)  # Combine and drop duplicates\n",
        "    except FileNotFoundError:\n",
        "        pass  # If the file doesn't exist, just proceed to save the new data\n",
        "\n",
        "    # Save the combined data to CSV\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "# Run the scraper\n",
        "combined_articles = combine_articles()\n",
        "\n",
        "# Validate the articles\n",
        "validated_articles = validate_articles(combined_articles)\n",
        "\n",
        "# Create a DataFrame from the validated articles\n",
        "df = pd.DataFrame(validated_articles, columns=[\n",
        "    'Date', 'Headline', 'Description', 'Key Highlights', 'Link', 'Source Name', 'Blog', 'Blog Writer'\n",
        "])\n",
        "\n",
        "# Convert 'Date' column to datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Filter articles from the past 30 days\n",
        "df = df[df['Date'] >= pd.to_datetime(thirty_days_ago)]\n",
        "\n",
        "# Drop rows with NaN values in the 'Date' column\n",
        "df.dropna(subset=['Date'], inplace=True)\n",
        "\n",
        "# Sort the DataFrame by Date in ascending order\n",
        "df = df.sort_values(by='Date', ascending=True)\n",
        "\n",
        "# Merge the 'Date' column for articles on the same date\n",
        "last_date = None\n",
        "for i, article in df.iterrows():\n",
        "    if article['Date'].date() == last_date:\n",
        "        df.at[i, 'Date'] = ''  # Empty string for merged dates\n",
        "    else:\n",
        "        last_date = article['Date'].date()  # Update last_date to the current article's date\n",
        "\n",
        "# Save the DataFrame to CSV to persist data\n",
        "csv_filename = 'cybersecurity_articles.csv'\n",
        "save_to_csv(df, csv_filename)\n",
        "\n",
        "# Convert 'Date' column to string format for Google Sheets\n",
        "df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "# Authenticate and write to Google Sheets\n",
        "def update_google_sheet(df, sheet_name):\n",
        "    SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
        "\n",
        "    # Load credentials from the service account file\n",
        "    creds = Credentials.from_service_account_file('credentials.json', scopes=SCOPES)\n",
        "\n",
        "    # Authenticate and open the sheet\n",
        "    client = gspread.authorize(creds)\n",
        "    sheet = client.open(sheet_name).sheet1  # Assuming you're writing to the first sheet\n",
        "\n",
        "    # Ensure all values are serializable\n",
        "    df.fillna('', inplace=True)  # Replace NaN with empty strings\n",
        "\n",
        "    # Convert the DataFrame to a list of lists\n",
        "    data = [df.columns.values.tolist()] + df.values.tolist()\n",
        "\n",
        "    # Clear the sheet and insert the new data\n",
        "    sheet.clear()\n",
        "    sheet.insert_rows(data, 1)  # Insert starting at the first row\n",
        "\n",
        "# Update your Google Sheet\n",
        "update_google_sheet(df, 'NA_new')\n"
      ]
    }
  ]
}